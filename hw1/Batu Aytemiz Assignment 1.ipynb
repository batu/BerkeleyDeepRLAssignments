{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batu Aytemiz Assignment 1\n",
    "\n",
    "In this assignment we:\n",
    "0. Create a dataset from given expert policies.\n",
    "1. Use the dataset to train and test a policy. (Behavior Cloning)\n",
    "2. Implement DAGGER (Imitation Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_util\n",
    "import gym\n",
    "import load_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Behavior Cloning\n",
    "### I have created the behivor data by running the given expert policies and saving the (state - action distribution) pairs. In this part I am creating a simple neural network to predict the action distribution given a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points gathered from running the given expert policy.\n",
      "The state space is of size: 17.\n",
      "The action space is of size: 6.\n"
     ]
    }
   ],
   "source": [
    "names = [\"HalfCheetah-v2\", \"Ant-v2\", \"Humanoid-v2\", \"Walker2d-v2\"]\n",
    "ENV_NAME = names[-1]\n",
    "\n",
    "def load_data():\n",
    "    with open(f\"bc_data/bc_data_{ENV_NAME}\", \"rb\") as data_file:\n",
    "        raw_data = pickle.load(data_file)\n",
    "\n",
    "    x_train = raw_data[0]\n",
    "    y_train_raw = raw_data[1]\n",
    "    y_train_raw = y_train_raw.squeeze()\n",
    "    return x_train, y_train_raw\n",
    "\n",
    "x_train, y_train = load_data()\n",
    "state_space = x_train.shape[1]\n",
    "action_space = y_train.shape[1]\n",
    "\n",
    "\n",
    "print(f\"There are {len(x_train)} data points gathered from running the given expert policy.\")\n",
    "print(f\"The state space is of size: {state_space}.\")\n",
    "print(f\"The action space is of size: {action_space}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am using a very simple NN architecture as our state and action spaces are relatively simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    hidden_layer_size = 64\n",
    "    tf.reset_default_graph()\n",
    "    input_ph = tf.placeholder(dtype=tf.float32, shape=[None, state_space])\n",
    "    output_ph = tf.placeholder(dtype=tf.float32, shape=[None, action_space])\n",
    "    \n",
    "    w0 = tf.get_variable(name='W0', shape=[state_space, hidden_layer_size],  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    w1 = tf.get_variable(name='W1', shape=[hidden_layer_size, hidden_layer_size],           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    w2 = tf.get_variable(name='W2', shape=[hidden_layer_size, action_space], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b0 = tf.get_variable(name=\"b0\", shape=[hidden_layer_size], initializer=tf.constant_initializer(0.))\n",
    "    b1 = tf.get_variable(name=\"b1\", shape=[hidden_layer_size], initializer=tf.constant_initializer(0.))\n",
    "    b2 = tf.get_variable(name=\"b2\", shape=[action_space], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "    weights = [w0, w1, w2]\n",
    "    biases = [b0, b1, b2]\n",
    "    activations = [tf.nn.relu, tf.nn.relu, None]\n",
    "    \n",
    "    layer = input_ph\n",
    "    for W, b, activation in zip(weights, biases, activations):\n",
    "        layer = tf.matmul(layer, W)\n",
    "        layer = layer + b\n",
    "        if activation is not None:\n",
    "            layer = activation(layer)\n",
    "    output_pred = layer\n",
    "    return input_ph, output_ph, output_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are training the model using the expert data.\n",
      "Training model with 10000 data points.\n",
      "0000 mse: 2.010\n",
      "1000 mse: 0.033\n",
      "2000 mse: 0.025\n",
      "3000 mse: 0.016\n",
      "4000 mse: 0.019\n",
      "5000 mse: 0.017\n",
      "6000 mse: 0.010\n",
      "7000 mse: 0.008\n",
      "8000 mse: 0.009\n",
      "9000 mse: 0.010\n",
      "10000 mse: 0.009\n",
      "Final mse: 0.009\n"
     ]
    }
   ],
   "source": [
    "def train_model(x_train=x_train, y_train=y_train, imitation=False):\n",
    "    input_ph, output_ph, output_pred = create_model()\n",
    "    print(f\"Training model with {len(x_train)} data points.\")\n",
    "    with tf.Session() as sess:\n",
    "        # create saver to save model variables\n",
    "        saver = tf.train.Saver()\n",
    "        mse = tf.reduce_mean(0.5 * tf.square(output_pred - output_ph))\n",
    "\n",
    "        # create optimizer\n",
    "        opt = tf.train.AdamOptimizer().minimize(mse)\n",
    "\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if imitation:\n",
    "            saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        \n",
    "        \n",
    "        batch_size = 32\n",
    "        for training_step in range(10001):\n",
    "            # get a random subset of the training data\n",
    "            indices = np.random.randint(low=0, high=len(x_train), size=batch_size)\n",
    "            input_batch = x_train[indices]\n",
    "            output_batch = y_train[indices]\n",
    "\n",
    "            # run the optimizer and get the mse\n",
    "            _, mse_run = sess.run([opt, mse], feed_dict={input_ph: input_batch, output_ph: output_batch})\n",
    "            # print the mse every so often\n",
    "            if training_step % 1000 == 0 and not imitation:\n",
    "                print('{0:04d} mse: {1:.3f}'.format(training_step, mse_run))\n",
    "                saver.save(sess, '/tmp/model.ckpt')\n",
    "    print(f\"Final mse: {mse_run:.3f}\")\n",
    "print(\"We are training the model using the expert data.\")\n",
    "train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "iter 0\n",
      "100/10000\n",
      "The total rewards that has been accumulated is: 75.55992480680706\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    input_ph, output_ph, output_pred = create_model()\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "\n",
    "        env = gym.make(ENV_NAME)\n",
    "        max_steps = 10000\n",
    "\n",
    "        for i in range(1):\n",
    "            print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                actions = output_pred_run = sess.run(output_pred, feed_dict={input_ph: (obs,)})\n",
    "                action = np.argmax(actions)\n",
    "                obs, r, done, _ = env.step(action)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "        print(f\"The total rewards that has been accumulated is: {totalr}\")\n",
    "        \n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not surprisingly the model has done terrilby. The agent has diverged from the training data (by falling over) allmost immediately and then kept on predicting the same action which never allowed it to get up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can test other enviroments but the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points gathered from running the given expert policy.\n",
      "The state space is of size: 376.\n",
      "The action space is of size: 17.\n",
      "\n",
      " Training of Humanoid-v2 has started.\n",
      "Training model with 10000 data points.\n",
      "0000 mse: 119.905\n",
      "1000 mse: 0.320\n",
      "2000 mse: 0.191\n",
      "3000 mse: 0.210\n",
      "4000 mse: 0.173\n",
      "5000 mse: 0.143\n",
      "6000 mse: 0.174\n",
      "7000 mse: 0.139\n",
      "8000 mse: 0.151\n",
      "9000 mse: 0.108\n",
      "10000 mse: 0.119\n",
      "Final mse: 0.119\n",
      "Training started.\n",
      "\n",
      "Testing started.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "iter 0\n",
      "The total rewards that has been accumulated is: 238.74245546199305\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"Humanoid-v2\"\n",
    "    \n",
    "x_train, y_train = load_data()\n",
    "state_space = x_train.shape[1]\n",
    "action_space = y_train.shape[1]\n",
    "\n",
    "print(f\"There are {len(x_train)} data points gathered from running the given expert policy.\")\n",
    "print(f\"The state space is of size: {state_space}.\")\n",
    "print(f\"The action space is of size: {action_space}.\")\n",
    "\n",
    "print(f\"\\n Training of {ENV_NAME} has started.\")\n",
    "train_model(x_train, y_train)\n",
    "print(\"Training started.\\n\")\n",
    "\n",
    "print(\"Testing started.\")\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_current_policy(rollout_count=10):\n",
    "    input_ph, output_ph, output_pred = create_model()\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "\n",
    "        env = gym.make(ENV_NAME)\n",
    "        max_steps = 10000\n",
    "\n",
    "        new_states = []\n",
    "        for i in range(rollout_count):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                actions = output_pred_run = sess.run(output_pred, feed_dict={input_ph: (obs,)})\n",
    "                action = np.argmax(actions)\n",
    "                obs, r, done, _ = env.step(action)\n",
    "                new_states.append(obs)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                # if steps % 100 == 0: print(\"Steps: %i/%i with reward: %i\"%(steps, max_steps,totalr))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "        return new_states, totalr\n",
    "\n",
    "def get_labels_from_expert(unlabeled_data_set):\n",
    "    with tf.Session() as sess:\n",
    "        # {ENV_NAME}.pkl\n",
    "        expert_policy_fn = load_policy.load_policy(f\"experts/HalfCheetah-v2.pkl\")\n",
    "        action_labels = [expert_policy_fn(unlabeled_data[None,:]) for unlabeled_data in unlabeled_data_set]\n",
    "        return np.array(action_labels).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 10000 data points.\n",
      "0000 mse: 2.093\n",
      "1000 mse: 0.047\n",
      "2000 mse: 0.024\n",
      "3000 mse: 0.027\n",
      "4000 mse: 0.022\n",
      "5000 mse: 0.017\n",
      "6000 mse: 0.016\n",
      "7000 mse: 0.011\n",
      "8000 mse: 0.010\n",
      "9000 mse: 0.009\n",
      "10000 mse: 0.006\n",
      "Final mse: 0.006\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "obs (1, 17) (1, 17)\n",
      "Training model with 12152 data points.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Final mse: 0.004\n",
      "The reward in iteration 1 is: 73.9825055206486\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "obs (1, 17) (1, 17)\n",
      "Training model with 14029 data points.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"Walker2d-v2\"\n",
    "\n",
    "x_train, y_train = load_data()\n",
    "state_space = x_train.shape[1]\n",
    "action_space = y_train.shape[1]\n",
    "\n",
    "def run_imitation_learning(x_train, y_train, imitation_count = 99):\n",
    "    train_model()\n",
    "    for i in range(imitation_count):\n",
    "        new_data_set, reward = get_dataset_from_current_policy()\n",
    "        new_label_set = get_labels_from_expert(new_data_set)\n",
    "        x_train = np.concatenate((x_train, new_data_set), axis=0)\n",
    "        y_train = np.concatenate((y_train, new_label_set), axis=0)\n",
    "        train_model(x_train, y_train, imitation=True)\n",
    "        print(f\"The reward in iteration {i+1} is: {reward}\")\n",
    "run_imitation_learning(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
